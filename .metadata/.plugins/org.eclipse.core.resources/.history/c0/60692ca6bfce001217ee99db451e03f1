package edu.brown.cs.ai.experiments.planning.fourrooms;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import edu.brown.cs.ai.behavior.oomdp.optiondiscovery.macros.AllSequenceExtraction;
import edu.brown.cs.ai.behavior.oomdp.options.MacroAction;
import edu.brown.cs.ai.behavior.oomdp.options.OptionEvaluatingRF;
import edu.brown.cs.ai.behavior.oomdp.planning.StateConditionTest;
import edu.brown.cs.ai.behavior.oomdp.planning.deterministc.DeterministicPlanner;
import edu.brown.cs.ai.behavior.oomdp.planning.deterministc.SDPlannerPolicy;
import edu.brown.cs.ai.behavior.oomdp.planning.deterministc.SinglePFTF;
import edu.brown.cs.ai.behavior.oomdp.planning.deterministc.TFGoalCondition;
import edu.brown.cs.ai.behavior.oomdp.planning.deterministc.UniformCostRF;
import edu.brown.cs.ai.behavior.oomdp.planning.deterministc.uninformed.dfs.DFS;
import edu.brown.cs.ai.domain.oomdp.fourrooms.FourRoomsDomain;
import edu.umbc.cs.maple.behavior.oomdp.Policy;
import edu.umbc.cs.maple.oomdp.Attribute;
import edu.umbc.cs.maple.oomdp.Domain;
import edu.umbc.cs.maple.oomdp.RewardFunction;
import edu.umbc.cs.maple.oomdp.State;
import edu.umbc.cs.maple.oomdp.TerminalFunction;

public class FourRoomsMacrosExp {

	Domain 									d;
	Map <String, List<Attribute>> 			attributesForHashCode;
	TerminalFunction 						tf;
	StateConditionTest						gc;
	RewardFunction							rf;
	
	/**
	 * @param args
	 */
	public static void main(String[] args) {
		
		FourRoomsMacrosExp exp = new FourRoomsMacrosExp();

	}
	
	public FourRoomsMacrosExp(){
	
		d = (new FourRoomsDomain()).generateDomain();
		
		attributesForHashCode = new HashMap<String, List<Attribute>>();
		
		List <Attribute> agentAtts = new ArrayList<Attribute>();
		agentAtts.add(d.getAttribute(FourRoomsDomain.ATTX));
		agentAtts.add(d.getAttribute(FourRoomsDomain.ATTY));
		attributesForHashCode.put(FourRoomsDomain.CLASSAGENT, agentAtts);
		
		List <Attribute> goalAtts = new ArrayList<Attribute>(agentAtts);
		attributesForHashCode.put(FourRoomsDomain.CLASSGOAL, goalAtts);
		
		tf = new SinglePFTF(d.getPropFunction(FourRoomsDomain.PFATGOAL));
		gc = new TFGoalCondition(tf);
		rf = new OptionEvaluatingRF(new UniformCostRF());
		
	}
	
	
	public List<MacroAction> getMacroActionsFor(int ax, int ay, int gx, int gy, List<MacroAction> existingMacros){
		
		State s = FourRoomsDomain.getCleanState();
		FourRoomsDomain.setAgent(s, ax, ay);
		FourRoomsDomain.setGoal(s, gx, gy);
		
		Policy p = this.getPlan(s, existingMacros);
		
		
		return AllSequenceExtraction.getAllMacroSubsequences(4, s, p, tf);
	}
	
	
	public Policy getPlan(State s, List<MacroAction> existingMacros){
		
		DFS planner = new DFS(d, rf, gc, attributesForHashCode, 5, false, false);
		Policy p = new SDPlannerPolicy((DeterministicPlanner)planner);
		
		for(MacroAction ma : existingMacros){
			planner.addNonDomainReferencedAction(ma);
		}
		
		planner.planFromState(s);
		
		
		return p;
	}

}
